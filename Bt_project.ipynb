{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled22.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KSg6lWVkNSc"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import shutil\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "matplotlib.use(u'nbAgg')\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn import preprocessing\n",
        "import pandas as pd\n",
        "from multiprocessing import Process# this is used for multithreading\n",
        "import multiprocessing\n",
        "import codecs# this is used for file operations \n",
        "import random as r\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#separating byte files and asm files \n",
        "\n",
        "source = 'train'\n",
        "destination = 'byteFiles'\n",
        "\n",
        "# we will check if the folder 'byteFiles' exists if it not there we will create a folder with the same name\n",
        "if not os.path.isdir(destination):\n",
        "    os.makedirs(destination)\n",
        "if os.path.isdir(source):\n",
        "    os.rename(source,'asmFiles')\n",
        "    source='asmFiles'\n",
        "    data_files = os.listdir(source)\n",
        "    for file in asm_files:\n",
        "        if (file.endswith(\"bytes\")):\n",
        "            shutil.move(source+file,destination)"
      ],
      "metadata": {
        "id": "Oh0s2E2okxJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y=pd.read_csv(\"trainLabels.csv\")\n",
        "total = len(Y)*1.\n",
        "ax=sns.countplot(x=\"Class\", data=Y)\n",
        "for p in ax.patches:\n",
        "        ax.annotate('{:.1f}%'.format(100*p.get_height()/total), (p.get_x()+0.1, p.get_height()+5))\n",
        "ax.yaxis.set_ticks(np.linspace(0, total, 11))\n",
        " \n",
        "ax.set_yticklabels(map('{:.1f}%'.format, 100*ax.yaxis.get_majorticklocs()/total))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yu8FiNaRk4sT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files=os.listdir('byteFiles')\n",
        "filenames=Y['Id'].tolist()\n",
        "class_y=Y['Class'].tolist()\n",
        "class_bytes=[]\n",
        "sizebytes=[]\n",
        "fnames=[]\n",
        "for file in files:\n",
        "    statinfo=os.stat('byteFiles/'+file)\n",
        "    # split the file name at '.' and take the first part of it i.e the file name\n",
        "    file=file.split('.')[0]\n",
        "    if any(file == filename for filename in filenames):\n",
        "        i=filenames.index(file)\n",
        "        class_bytes.append(class_y[i])\n",
        "        # converting into Mb's\n",
        "        sizebytes.append(statinfo.st_size/(1024.0*1024.0))\n",
        "        fnames.append(file)\n",
        "data_size_byte=pd.DataFrame({'ID':fnames,'size':sizebytes,'Class':class_bytes})\n",
        "print (data_size_byte.head())"
      ],
      "metadata": {
        "id": "iJ-IoO5Wk8kR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ax = sns.boxplot(x=\"Class\", y=\"size\", data=data_size_byte)\n",
        "plt.title(\"boxplot of .bytes file sizes\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OXr-MaO8lEXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = os.listdir('byteFiles')\n",
        "filenames=[]\n",
        "array=[]\n",
        "for file in files:\n",
        "    if(f.endswith(\"bytes\")):\n",
        "        file=file.split('.')[0]\n",
        "        text_file = open('byteFiles/'+file+\".txt\", 'w+')\n",
        "        with open('byteFiles/'+file,\"r\") as fp:\n",
        "            lines=\"\"\n",
        "            for line in fp:\n",
        "                a=line.rstrip().split(\" \")[1:]\n",
        "                b=' '.join(a)\n",
        "                b=b+\"\\n\"\n",
        "                text_file.write(b)\n",
        "            fp.close()\n",
        "            os.remove('byteFiles/'+file)\n",
        "        text_file.close()\n",
        "\n",
        "files = os.listdir('byteFiles')\n",
        "filenames2=[]\n",
        "feature_matrix = np.zeros((len(files),257),dtype=int)\n",
        "k=0\n",
        "byte_feature_file=open('result.csv','w+')\n",
        "byte_feature_file.write(\"ID,0,1,2,3,4,5,6,7,8,9,0a,0b,0c,0d,0e,0f,10,11,12,13,14,15,16,17,18,19,1a,1b,1c,1d,1e,1f,20,21,22,23,24,25,26,27,28,29,2a,2b,2c,2d,2e,2f,30,31,32,33,34,35,36,37,38,39,3a,3b,3c,3d,3e,3f,40,41,42,43,44,45,46,47,48,49,4a,4b,4c,4d,4e,4f,50,51,52,53,54,55,56,57,58,59,5a,5b,5c,5d,5e,5f,60,61,62,63,64,65,66,67,68,69,6a,6b,6c,6d,6e,6f,70,71,72,73,74,75,76,77,78,79,7a,7b,7c,7d,7e,7f,80,81,82,83,84,85,86,87,88,89,8a,8b,8c,8d,8e,8f,90,91,92,93,94,95,96,97,98,99,9a,9b,9c,9d,9e,9f,a0,a1,a2,a3,a4,a5,a6,a7,a8,a9,aa,ab,ac,ad,ae,af,b0,b1,b2,b3,b4,b5,b6,b7,b8,b9,ba,bb,bc,bd,be,bf,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9,ca,cb,cc,cd,ce,cf,d0,d1,d2,d3,d4,d5,d6,d7,d8,d9,da,db,dc,dd,de,df,e0,e1,e2,e3,e4,e5,e6,e7,e8,e9,ea,eb,ec,ed,ee,ef,f0,f1,f2,f3,f4,f5,f6,f7,f8,f9,fa,fb,fc,fd,fe,ff,??\")\n",
        "for file in files:\n",
        "    filenames2.append(f)\n",
        "    byte_feature_file.write(file+\",\")\n",
        "    if(file.endswith(\"txt\")):\n",
        "        with open('byteFiles/'+file,\"r\") as byte_flie:\n",
        "            for lines in byte_flie:\n",
        "                line=lines.rstrip().split(\" \")\n",
        "                for hex_code in line:\n",
        "                    if hex_code=='??':\n",
        "                        feature_matrix[k][256]+=1\n",
        "                    else:\n",
        "                        feature_matrix[k][int(hex_code,16)]+=1\n",
        "        byte_flie.close()\n",
        "    for i in feature_matrix[k]:\n",
        "        byte_feature_file.write(str(i)+\",\")\n",
        "    byte_feature_file.write(\"\\n\")\n",
        "    \n",
        "    k += 1\n",
        "\n",
        "byte_feature_file.close()"
      ],
      "metadata": {
        "id": "VljKE0WNlF__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(df):\n",
        "    result1 = df.copy()\n",
        "    for feature_name in df.columns:\n",
        "        if (str(feature_name) != str('ID') and str(feature_name)!=str('Class')):\n",
        "            max_value = df[feature_name].max()\n",
        "            min_value = df[feature_name].min()\n",
        "            result1[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
        "    return result1\n",
        "result = normalize(result)"
      ],
      "metadata": {
        "id": "pLSOf0iblPFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xtsne=TSNE(perplexity=50)\n",
        "results=xtsne.fit_transform(result.drop(['ID','Class'], axis=1))\n",
        "vis_x = results[:, 0]\n",
        "vis_y = results[:, 1]\n",
        "plt.scatter(vis_x, vis_y, c=data_y, cmap=plt.cm.get_cmap(\"jet\", 9))\n",
        "plt.colorbar(ticks=range(10))\n",
        "plt.clim(0.5, 9)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sCO1VXlplTq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xtsne=TSNE(perplexity=30)\n",
        "results=xtsne.fit_transform(result.drop(['ID','Class'], axis=1))\n",
        "vis_x = results[:, 0]\n",
        "vis_y = results[:, 1]\n",
        "plt.scatter(vis_x, vis_y, c=data_y, cmap=plt.cm.get_cmap(\"jet\", 9))\n",
        "plt.colorbar(ticks=range(10))\n",
        "plt.clim(0.5, 9)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tjNUMpL9lWnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_y = result['Class']\n",
        "# split the data into test and train by maintaining same distribution of output varaible 'y_true' [stratify=y_true]\n",
        "X_train, X_test, y_train, y_test = train_test_split(result.drop(['ID','Class'], axis=1), data_y,stratify=data_y,test_size=0.20)\n",
        "X_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train,stratify=y_train,test_size=0.20)"
      ],
      "metadata": {
        "id": "_LS4HPR6lZUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_class_distribution = y_train.value_counts().sortlevel()\n",
        "test_class_distribution = y_test.value_counts().sortlevel()\n",
        "cv_class_distribution = y_cv.value_counts().sortlevel()\n",
        "\n",
        "my_colors = 'rgbkymc'\n",
        "train_class_distribution.plot(kind='bar', color=my_colors)\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Data points per Class')\n",
        "plt.title('Distribution of yi in train data')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# ref: argsort https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html\n",
        "# -(train_class_distribution.values): the minus sign will give us in decreasing order\n",
        "sorted_yi = np.argsort(-train_class_distribution.values)\n",
        "for i in sorted_yi:\n",
        "    print('Number of data points in class', i+1, ':',train_class_distribution.values[i], '(', np.round((train_class_distribution.values[i]/y_train.shape[0]*100), 3), '%)')\n",
        "\n",
        "    \n",
        "print('-'*80)\n",
        "my_colors = 'rgbkymc'\n",
        "test_class_distribution.plot(kind='bar', color=my_colors)\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Data points per Class')\n",
        "plt.title('Distribution of yi in test data')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "sorted_yi = np.argsort(-test_class_distribution.values)\n",
        "for i in sorted_yi:\n",
        "    print('Number of data points in class', i+1, ':',test_class_distribution.values[i], '(', np.round((test_class_distribution.values[i]/y_test.shape[0]*100), 3), '%)')\n",
        "\n",
        "print('-'*80)\n",
        "my_colors = 'rgbkymc'\n",
        "cv_class_distribution.plot(kind='bar', color=my_colors)\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Data points per Class')\n",
        "plt.title('Distribution of yi in cross validation data')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "sorted_yi = np.argsort(-train_class_distribution.values)\n",
        "for i in sorted_yi:\n",
        "    print('Number of data points in class', i+1, ':',cv_class_distribution.values[i], '(', np.round((cv_class_distribution.values[i]/y_cv.shape[0]*100), 3), '%)')\n"
      ],
      "metadata": {
        "id": "9O5kvhDSluMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "test_data_len = X_test.shape[0]\n",
        "cv_data_len = X_cv.shape[0]\n",
        "cv_predicted_y = np.zeros((cv_data_len,9))\n",
        "for i in range(cv_data_len):\n",
        "    rand_probs = np.random.rand(1,9)\n",
        "    cv_predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])\n",
        "print(\"Log loss on Cross Validation Data using Random Model\",log_loss(y_cv,cv_predicted_y, eps=1e-15))\n",
        "\n",
        "test_predicted_y = np.zeros((test_data_len,9))\n",
        "for i in range(test_data_len):\n",
        "    rand_probs = np.random.rand(1,9)\n",
        "    test_predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])\n",
        "print(\"Log loss on Test Data using Random Model\",log_loss(y_test,test_predicted_y, eps=1e-15))\n",
        "\n",
        "predicted_y =np.argmax(test_predicted_y, axis=1)\n",
        "plot_confusion_matrix(y_test, predicted_y+1)"
      ],
      "metadata": {
        "id": "GzV-uxCKlyFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#knn classifier\n",
        "ror_array[i]))\n",
        "plt.grid()\n",
        "plt.title(\"Cross Validation Error for each alpha\")\n",
        "plt.xlabel(\"Alpha i's\")\n",
        "plt.ylabel(\"Error measure\")\n",
        "plt.show()\n",
        "\n",
        "k_cfl=KNeighborsClassifier(n_neighbors=alpha[best_alpha])\n",
        "k_cfl.fit(X_train,y_train)\n",
        "sig_clf = CalibratedClassifierCV(k_cfl, method=\"sigmoid\")alpha = [x for x in range(1, 15, 2)]\n",
        "cv_log_error_array=[]\n",
        "for i in alpha:\n",
        "    k_cfl=KNeighborsClassifier(n_neighbors=i)\n",
        "    k_cfl.fit(X_train,y_train)\n",
        "    sig_clf = CalibratedClassifierCV(k_cfl, method=\"sigmoid\")\n",
        "    sig_clf.fit(X_train, y_train)\n",
        "    predict_y = sig_clf.predict_proba(X_cv)\n",
        "    cv_log_error_array.append(log_loss(y_cv, predict_y, labels=k_cfl.classes_, eps=1e-15))\n",
        "    \n",
        "for i in range(len(cv_log_error_array)):\n",
        "    print ('log_loss for k = ',alpha[i],'is',cv_log_error_array[i])\n",
        "\n",
        "best_alpha = np.argmin(cv_log_error_array)\n",
        "    \n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(alpha, cv_log_error_array,c='g')\n",
        "for i, txt in enumerate(np.round(cv_log_error_array,3)):\n",
        "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_er\n",
        "sig_clf.fit(X_train, y_train)\n",
        "    \n",
        "predict_y = sig_clf.predict_proba(X_train)\n",
        "print ('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y))\n",
        "predict_y = sig_clf.predict_proba(X_cv)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y))\n",
        "predict_y = sig_clf.predict_proba(X_test)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y))\n",
        "plot_confusion_matrix(y_test, sig_clf.predict(X_test))"
      ],
      "metadata": {
        "id": "4oRJIad6l6t7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#random forest classifier\n",
        "alpha=[10,50,100,500,1000,2000,3000]\n",
        "cv_log_error_array=[]\n",
        "train_log_error_array=[]\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "for i in alpha:\n",
        "    r_cfl=RandomForestClassifier(n_estimators=i,random_state=42,n_jobs=-1)\n",
        "    r_cfl.fit(X_train,y_train)\n",
        "    sig_clf = CalibratedClassifierCV(r_cfl, method=\"sigmoid\")\n",
        "    sig_clf.fit(X_train, y_train)\n",
        "    predict_y = sig_clf.predict_proba(X_cv)\n",
        "    cv_log_error_array.append(log_loss(y_cv, predict_y, labels=r_cfl.classes_, eps=1e-15))\n",
        "\n",
        "for i in range(len(cv_log_error_array)):\n",
        "    print ('log_loss for c = ',alpha[i],'is',cv_log_error_array[i])\n",
        "\n",
        "\n",
        "best_alpha = np.argmin(cv_log_error_array)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(alpha, cv_log_error_array,c='g')\n",
        "for i, txt in enumerate(np.round(cv_log_error_array,3)):\n",
        "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\n",
        "plt.grid()\n",
        "plt.title(\"Cross Validation Error for each alpha\")\n",
        "plt.xlabel(\"Alpha i's\")\n",
        "plt.ylabel(\"Error measure\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "r_cfl=RandomForestClassifier(n_estimators=alpha[best_alpha],random_state=42,n_jobs=-1)\n",
        "r_cfl.fit(X_train,y_train)\n",
        "sig_clf = CalibratedClassifierCV(r_cfl, method=\"sigmoid\")\n",
        "sig_clf.fit(X_train, y_train)\n",
        "\n",
        "predict_y = sig_clf.predict_proba(X_train)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y))\n",
        "predict_y = sig_clf.predict_proba(X_cv)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y))\n",
        "predict_y = sig_clf.predict_proba(X_test)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y))\n",
        "plot_confusion_matrix(y_test, sig_clf.predict(X_test))"
      ],
      "metadata": {
        "id": "CdB3XLM5mQCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#xg boost classifier\n",
        "alpha=[10,50,100,500,1000,2000]\n",
        "cv_log_error_array=[]\n",
        "for i in alpha:\n",
        "    x_cfl=XGBClassifier(n_estimators=i,nthread=-1)\n",
        "    x_cfl.fit(X_train,y_train)\n",
        "    sig_clf = CalibratedClassifierCV(x_cfl, method=\"sigmoid\")\n",
        "    sig_clf.fit(X_train, y_train)\n",
        "    predict_y = sig_clf.predict_proba(X_cv)\n",
        "    cv_log_error_array.append(log_loss(y_cv, predict_y, labels=x_cfl.classes_, eps=1e-15))\n",
        "\n",
        "for i in range(len(cv_log_error_array)):\n",
        "    print ('log_loss for c = ',alpha[i],'is',cv_log_error_array[i])\n",
        "\n",
        "\n",
        "best_alpha = np.argmin(cv_log_error_array)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(alpha, cv_log_error_array,c='g')\n",
        "for i, txt in enumerate(np.round(cv_log_error_array,3)):\n",
        "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\n",
        "plt.grid()\n",
        "plt.title(\"Cross Validation Error for each alpha\")\n",
        "plt.xlabel(\"Alpha i's\")\n",
        "plt.ylabel(\"Error measure\")\n",
        "plt.show()\n",
        "\n",
        "x_cfl=XGBClassifier(n_estimators=alpha[best_alpha],nthread=-1)\n",
        "x_cfl.fit(X_train,y_train)\n",
        "sig_clf = CalibratedClassifierCV(x_cfl, method=\"sigmoid\")\n",
        "sig_clf.fit(X_train, y_train)\n",
        "    \n",
        "predict_y = sig_clf.predict_proba(X_train)\n",
        "print ('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y))\n",
        "predict_y = sig_clf.predict_proba(X_cv)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y))\n",
        "predict_y = sig_clf.predict_proba(X_test)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y))\n",
        "plot_confusion_matrix(y_test, sig_clf.predict(X_test))"
      ],
      "metadata": {
        "id": "YuEkGyoKmfFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WhLo-fPimnoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = [10 ** x for x in range(-5, 4)]\n",
        "cv_log_error_array=[]\n",
        "for i in alpha:\n",
        "    logisticR=LogisticRegression(penalty='l2',C=i,class_weight='balanced')\n",
        "    logisticR.fit(X_train,y_train)\n",
        "    sig_clf = CalibratedClassifierCV(logisticR, method=\"sigmoid\")\n",
        "    sig_clf.fit(X_train, y_train)\n",
        "    predict_y = sig_clf.predict_proba(X_cv)\n",
        "    cv_log_error_array.append(log_loss(y_cv, predict_y, labels=logisticR.classes_, eps=1e-15))\n",
        "    \n",
        "for i in range(len(cv_log_error_array)):\n",
        "    print ('log_loss for c = ',alpha[i],'is',cv_log_error_array[i])\n",
        "\n",
        "best_alpha = np.argmin(cv_log_error_array)\n",
        "    \n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(alpha, cv_log_error_array,c='g')\n",
        "for i, txt in enumerate(np.round(cv_log_error_array,3)):\n",
        "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\n",
        "plt.grid()\n",
        "plt.title(\"Cross Validation Error for each alpha\")\n",
        "plt.xlabel(\"Alpha i's\")\n",
        "plt.ylabel(\"Error measure\")\n",
        "plt.show()\n",
        "\n",
        "logisticR=LogisticRegression(penalty='l2',C=alpha[best_alpha],class_weight='balanced')\n",
        "logisticR.fit(X_train,y_train)\n",
        "sig_clf = CalibratedClassifierCV(logisticR, method=\"sigmoid\")\n",
        "sig_clf.fit(X_train, y_train)\n",
        "pred_y=sig_clf.predict(X_test)\n",
        "\n",
        "predict_y = sig_clf.predict_proba(X_train)\n",
        "print ('log loss for train data',log_loss(y_train, predict_y, labels=logisticR.classes_, eps=1e-15))\n",
        "predict_y = sig_clf.predict_proba(X_cv)\n",
        "print ('log loss for cv data',log_loss(y_cv, predict_y, labels=logisticR.classes_, eps=1e-15))\n",
        "predict_y = sig_clf.predict_proba(X_test)\n",
        "print ('log loss for test data',log_loss(y_test, predict_y, labels=logisticR.classes_, eps=1e-15))\n",
        "plot_confusion_matrix(y_test, sig_clf.predict(X_test))"
      ],
      "metadata": {
        "id": "dYiSyYkumI1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_cfl=XGBClassifier()\n",
        "\n",
        "prams={\n",
        "    'learning_rate':[0.01,0.03,0.05,0.1,0.15,0.2],\n",
        "     'n_estimators':[100,200,500,1000,2000],\n",
        "     'max_depth':[3,5,10],\n",
        "    'colsample_bytree':[0.1,0.3,0.5,1],\n",
        "    'subsample':[0.1,0.3,0.5,1]\n",
        "}\n",
        "random_cfl1=RandomizedSearchCV(x_cfl,param_distributions=prams,verbose=10,n_jobs=-1,)\n",
        "random_cfl1.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "QFjnPfKFnCO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training a hyper-parameter tuned Xg-Boost regressor on our train data\n",
        "x_cfl=XGBClassifier(n_estimators=2000, learning_rate=0.05, colsample_bytree=1, max_depth=3)\n",
        "x_cfl.fit(X_train,y_train)\n",
        "c_cfl=CalibratedClassifierCV(x_cfl,method='sigmoid')\n",
        "c_cfl.fit(X_train,y_train)\n",
        "\n",
        "predict_y = c_cfl.predict_proba(X_train)\n",
        "print ('train loss',log_loss(y_train, predict_y))\n",
        "predict_y = c_cfl.predict_proba(X_cv)\n",
        "print ('cv loss',log_loss(y_cv, predict_y))\n",
        "predict_y = c_cfl.predict_proba(X_test)\n",
        "print ('test loss',log_loss(y_test, predict_y))"
      ],
      "metadata": {
        "id": "YsUR5eo9nGcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#feature extraction\n",
        "folder_1 ='first'\n",
        "folder_2 ='second'\n",
        "folder_3 ='third'\n",
        "folder_4 ='fourth'\n",
        "folder_5 ='fifth'\n",
        "folder_6 = 'output'\n",
        "for i in [folder_1,folder_2,folder_3,folder_4,folder_5,folder_6]:\n",
        "    if not os.path.isdir(i):\n",
        "        os.makedirs(i)\n",
        "\n",
        "source='train/'\n",
        "files = os.listdir('train')\n",
        "ID=df['Id'].tolist()\n",
        "data=range(0,10868)\n",
        "r.shuffle(data)\n",
        "count=0\n",
        "for i in range(0,10868):\n",
        "    if i % 5==0:\n",
        "        shutil.move(source+files[data[i]],'first')\n",
        "    elif i%5==1:\n",
        "        shutil.move(source+files[data[i]],'second')\n",
        "    elif i%5 ==2:\n",
        "        shutil.move(source+files[data[i]],'thrid')\n",
        "    elif i%5 ==3:\n",
        "        shutil.move(source+files[data[i]],'fourth')\n",
        "    elif i%5==4:\n",
        "        shutil.move(source+files[data[i]],'fifth')"
      ],
      "metadata": {
        "id": "grYckQ2BnOEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def firstprocess():\n",
        " \n",
        "    prefixes = ['HEADER:','.text:','.Pav:','.idata:','.data:','.bss:','.rdata:','.edata:','.rsrc:','.tls:','.reloc:','.BSS:','.CODE']\n",
        "    #this are opcodes that are used to get best results\n",
        "    \n",
        "    opcodes = ['jmp', 'mov', 'retf', 'push', 'pop', 'xor', 'retn', 'nop', 'sub', 'inc', 'dec', 'add','imul', 'xchg', 'or', 'shr', 'cmp', 'call', 'shl', 'ror', 'rol', 'jnb','jz','rtn','lea','movzx']\n",
        "    #best keywords that are taken from different blogs\n",
        "    keywords = ['.dll','std::',':dword']\n",
        "    #Below taken registers are general purpose registers and special registers\n",
        "    #All the registers which are taken are best \n",
        "    registers=['edx','esi','eax','ebx','ecx','edi','ebp','esp','eip']\n",
        "    file1=open(\"output\\asmsmallfile.txt\",\"w+\")\n",
        "    files = os.listdir('first')\n",
        "    for f in files:\n",
        "        #filling the values with zeros into the arrays\n",
        "        prefixescount=np.zeros(len(prefixes),dtype=int)\n",
        "        opcodescount=np.zeros(len(opcodes),dtype=int)\n",
        "        keywordcount=np.zeros(len(keywords),dtype=int)\n",
        "        registerscount=np.zeros(len(registers),dtype=int)\n",
        "        features=[]\n",
        "        f2=f.split('.')[0]\n",
        "        file1.write(f2+\",\")\n",
        "        opcodefile.write(f2+\" \")\n",
        "       \n",
        "        with codecs.open('first/'+f,encoding='cp1252',errors ='replace') as fli:\n",
        "            for lines in fli:\n",
        "               \n",
        "                line=lines.rstrip().split()\n",
        "                l=line[0]\n",
        "                #counting the prefixs in each and every line\n",
        "                for i in range(len(prefixes)):\n",
        "                    if prefixes[i] in line[0]:\n",
        "                        prefixescount[i]+=1\n",
        "                line=line[1:]\n",
        "                #counting the opcodes in each and every line\n",
        "                for i in range(len(opcodes)):\n",
        "                    if any(opcodes[i]==li for li in line):\n",
        "                        features.append(opcodes[i])\n",
        "                        opcodescount[i]+=1\n",
        "                #counting registers in the line\n",
        "                for i in range(len(registers)):\n",
        "                    for li in line:\n",
        "                        # we will use registers only in 'text' and 'CODE' segments\n",
        "                        if registers[i] in li and ('text' in l or 'CODE' in l):\n",
        "                            registerscount[i]+=1\n",
        "                #counting keywords in the line\n",
        "                for i in range(len(keywords)):\n",
        "                    for li in line:\n",
        "                        if keywords[i] in li:\n",
        "                            keywordcount[i]+=1\n",
        "        #pushing the values into the file after reading whole file\n",
        "        for prefix in prefixescount:\n",
        "            file1.write(str(prefix)+\",\")\n",
        "        for opcode in opcodescount:\n",
        "            file1.write(str(opcode)+\",\")\n",
        "        for register in registerscount:\n",
        "            file1.write(str(register)+\",\")\n",
        "        for key in keywordcount:\n",
        "            file1.write(str(key)+\",\")\n",
        "        file1.write(\"\\n\")\n",
        "    file1.close()\n",
        "\n",
        "\n",
        "#same as above \n",
        "def secondprocess():\n",
        "    prefixes = ['HEADER:','.text:','.Pav:','.idata:','.data:','.bss:','.rdata:','.edata:','.rsrc:','.tls:','.reloc:','.BSS:','.CODE']\n",
        "    opcodes = ['jmp', 'mov', 'retf', 'push', 'pop', 'xor', 'retn', 'nop', 'sub', 'inc', 'dec', 'add','imul', 'xchg', 'or', 'shr', 'cmp', 'call', 'shl', 'ror', 'rol', 'jnb','jz','rtn','lea','movzx']\n",
        "    keywords = ['.dll','std::',':dword']\n",
        "    registers=['edx','esi','eax','ebx','ecx','edi','ebp','esp','eip']\n",
        "    file1=open(\"output\\mediumasmfile.txt\",\"w+\")\n",
        "    files = os.listdir('second')\n",
        "    for f in files:\n",
        "        prefixescount=np.zeros(len(prefixes),dtype=int)\n",
        "        opcodescount=np.zeros(len(opcodes),dtype=int)\n",
        "        keywordcount=np.zeros(len(keywords),dtype=int)\n",
        "        registerscount=np.zeros(len(registers),dtype=int)\n",
        "        features=[]\n",
        "        f2=f.split('.')[0]\n",
        "        file1.write(f2+\",\")\n",
        "        opcodefile.write(f2+\" \")\n",
        "        with codecs.open('second/'+f,encoding='cp1252',errors ='replace') as fli:\n",
        "            for lines in fli:\n",
        "                line=lines.rstrip().split()\n",
        "                l=line[0]\n",
        "                for i in range(len(prefixes)):\n",
        "                    if prefixes[i] in line[0]:\n",
        "                        prefixescount[i]+=1\n",
        "                line=line[1:]\n",
        "                for i in range(len(opcodes)):\n",
        "                    if any(opcodes[i]==li for li in line):\n",
        "                        features.append(opcodes[i])\n",
        "                        opcodescount[i]+=1\n",
        "                for i in range(len(registers)):\n",
        "                    for li in line:\n",
        "                        if registers[i] in li and ('text' in l or 'CODE' in l):\n",
        "                            registerscount[i]+=1\n",
        "                for i in range(len(keywords)):\n",
        "                    for li in line:\n",
        "                        if keywords[i] in li:\n",
        "                            keywordcount[i]+=1\n",
        "        for prefix in prefixescount:\n",
        "            file1.write(str(prefix)+\",\")\n",
        "        for opcode in opcodescount:\n",
        "            file1.write(str(opcode)+\",\")\n",
        "        for register in registerscount:\n",
        "            file1.write(str(register)+\",\")\n",
        "        for key in keywordcount:\n",
        "            file1.write(str(key)+\",\")\n",
        "        file1.write(\"\\n\")\n",
        "    file1.close()\n",
        "\n",
        "# same as smallprocess() functions\n",
        "def thirdprocess():\n",
        "    prefixes = ['HEADER:','.text:','.Pav:','.idata:','.data:','.bss:','.rdata:','.edata:','.rsrc:','.tls:','.reloc:','.BSS:','.CODE']\n",
        "    opcodes = ['jmp', 'mov', 'retf', 'push', 'pop', 'xor', 'retn', 'nop', 'sub', 'inc', 'dec', 'add','imul', 'xchg', 'or', 'shr', 'cmp', 'call', 'shl', 'ror', 'rol', 'jnb','jz','rtn','lea','movzx']\n",
        "    keywords = ['.dll','std::',':dword']\n",
        "    registers=['edx','esi','eax','ebx','ecx','edi','ebp','esp','eip']\n",
        "    file1=open(\"output\\largeasmfile.txt\",\"w+\")\n",
        "    files = os.listdir('thrid')\n",
        "    for f in files:\n",
        "        prefixescount=np.zeros(len(prefixes),dtype=int)\n",
        "        opcodescount=np.zeros(len(opcodes),dtype=int)\n",
        "        keywordcount=np.zeros(len(keywords),dtype=int)\n",
        "        registerscount=np.zeros(len(registers),dtype=int)\n",
        "        features=[]\n",
        "        f2=f.split('.')[0]\n",
        "        file1.write(f2+\",\")\n",
        "        opcodefile.write(f2+\" \")\n",
        "        with codecs.open('thrid/'+f,encoding='cp1252',errors ='replace') as fli:\n",
        "            for lines in fli:\n",
        "                line=lines.rstrip().split()\n",
        "                l=line[0]\n",
        "                for i in range(len(prefixes)):\n",
        "                    if prefixes[i] in line[0]:\n",
        "                        prefixescount[i]+=1\n",
        "                line=line[1:]\n",
        "                for i in range(len(opcodes)):\n",
        "                    if any(opcodes[i]==li for li in line):\n",
        "                        features.append(opcodes[i])\n",
        "                        opcodescount[i]+=1\n",
        "                for i in range(len(registers)):\n",
        "                    for li in line:\n",
        "                        if registers[i] in li and ('text' in l or 'CODE' in l):\n",
        "                            registerscount[i]+=1\n",
        "                for i in range(len(keywords)):\n",
        "                    for li in line:\n",
        "                        if keywords[i] in li:\n",
        "                            keywordcount[i]+=1\n",
        "        for prefix in prefixescount:\n",
        "            file1.write(str(prefix)+\",\")\n",
        "        for opcode in opcodescount:\n",
        "            file1.write(str(opcode)+\",\")\n",
        "        for register in registerscount:\n",
        "            file1.write(str(register)+\",\")\n",
        "        for key in keywordcount:\n",
        "            file1.write(str(key)+\",\")\n",
        "        file1.write(\"\\n\")\n",
        "    file1.close()\n",
        "\n",
        "\n",
        "def fourthprocess():\n",
        "    prefixes = ['HEADER:','.text:','.Pav:','.idata:','.data:','.bss:','.rdata:','.edata:','.rsrc:','.tls:','.reloc:','.BSS:','.CODE']\n",
        "    opcodes = ['jmp', 'mov', 'retf', 'push', 'pop', 'xor', 'retn', 'nop', 'sub', 'inc', 'dec', 'add','imul', 'xchg', 'or', 'shr', 'cmp', 'call', 'shl', 'ror', 'rol', 'jnb','jz','rtn','lea','movzx']\n",
        "    keywords = ['.dll','std::',':dword']\n",
        "    registers=['edx','esi','eax','ebx','ecx','edi','ebp','esp','eip']\n",
        "    file1=open(\"output\\hugeasmfile.txt\",\"w+\")\n",
        "    files = os.listdir('fourth/')\n",
        "    for f in files:\n",
        "        prefixescount=np.zeros(len(prefixes),dtype=int)\n",
        "        opcodescount=np.zeros(len(opcodes),dtype=int)\n",
        "        keywordcount=np.zeros(len(keywords),dtype=int)\n",
        "        registerscount=np.zeros(len(registers),dtype=int)\n",
        "        features=[]\n",
        "        f2=f.split('.')[0]\n",
        "        file1.write(f2+\",\")\n",
        "        opcodefile.write(f2+\" \")\n",
        "        with codecs.open('fourth/'+f,encoding='cp1252',errors ='replace') as fli:\n",
        "            for lines in fli:\n",
        "                line=lines.rstrip().split()\n",
        "                l=line[0]\n",
        "                for i in range(len(prefixes)):\n",
        "                    if prefixes[i] in line[0]:\n",
        "                        prefixescount[i]+=1\n",
        "                line=line[1:]\n",
        "                for i in range(len(opcodes)):\n",
        "                    if any(opcodes[i]==li for li in line):\n",
        "                        features.append(opcodes[i])\n",
        "                        opcodescount[i]+=1\n",
        "                for i in range(len(registers)):\n",
        "                    for li in line:\n",
        "                        if registers[i] in li and ('text' in l or 'CODE' in l):\n",
        "                            registerscount[i]+=1\n",
        "                for i in range(len(keywords)):\n",
        "                    for li in line:\n",
        "                        if keywords[i] in li:\n",
        "                            keywordcount[i]+=1\n",
        "        for prefix in prefixescount:\n",
        "            file1.write(str(prefix)+\",\")\n",
        "        for opcode in opcodescount:\n",
        "            file1.write(str(opcode)+\",\")\n",
        "        for register in registerscount:\n",
        "            file1.write(str(register)+\",\")\n",
        "        for key in keywordcount:\n",
        "            file1.write(str(key)+\",\")\n",
        "        file1.write(\"\\n\")\n",
        "    file1.close()\n",
        "\n",
        "\n",
        "def fifthprocess():\n",
        "    prefixes = ['HEADER:','.text:','.Pav:','.idata:','.data:','.bss:','.rdata:','.edata:','.rsrc:','.tls:','.reloc:','.BSS:','.CODE']\n",
        "    opcodes = ['jmp', 'mov', 'retf', 'push', 'pop', 'xor', 'retn', 'nop', 'sub', 'inc', 'dec', 'add','imul', 'xchg', 'or', 'shr', 'cmp', 'call', 'shl', 'ror', 'rol', 'jnb','jz','rtn','lea','movzx']\n",
        "    keywords = ['.dll','std::',':dword']\n",
        "    registers=['edx','esi','eax','ebx','ecx','edi','ebp','esp','eip']\n",
        "    file1=open(\"output\\trainasmfile.txt\",\"w+\")\n",
        "    files = os.listdir('fifth/')\n",
        "    for f in files:\n",
        "        prefixescount=np.zeros(len(prefixes),dtype=int)\n",
        "        opcodescount=np.zeros(len(opcodes),dtype=int)\n",
        "        keywordcount=np.zeros(len(keywords),dtype=int)\n",
        "        registerscount=np.zeros(len(registers),dtype=int)\n",
        "        features=[]\n",
        "        f2=f.split('.')[0]\n",
        "        file1.write(f2+\",\")\n",
        "        opcodefile.write(f2+\" \")\n",
        "        with codecs.open('fifth/'+f,encoding='cp1252',errors ='replace') as fli:\n",
        "            for lines in fli:\n",
        "                line=lines.rstrip().split()\n",
        "                l=line[0]\n",
        "                for i in range(len(prefixes)):\n",
        "                    if prefixes[i] in line[0]:\n",
        "                        prefixescount[i]+=1\n",
        "                line=line[1:]\n",
        "                for i in range(len(opcodes)):\n",
        "                    if any(opcodes[i]==li for li in line):\n",
        "                        features.append(opcodes[i])\n",
        "                        opcodescount[i]+=1\n",
        "                for i in range(len(registers)):\n",
        "                    for li in line:\n",
        "                        if registers[i] in li and ('text' in l or 'CODE' in l):\n",
        "                            registerscount[i]+=1\n",
        "                for i in range(len(keywords)):\n",
        "                    for li in line:\n",
        "                        if keywords[i] in li:\n",
        "                            keywordcount[i]+=1\n",
        "        for prefix in prefixescount:\n",
        "            file1.write(str(prefix)+\",\")\n",
        "        for opcode in opcodescount:\n",
        "            file1.write(str(opcode)+\",\")\n",
        "        for register in registerscount:\n",
        "            file1.write(str(register)+\",\")\n",
        "        for key in keywordcount:\n",
        "            file1.write(str(key)+\",\")\n",
        "        file1.write(\"\\n\")\n",
        "    file1.close()\n",
        "\n",
        "\n",
        "def main():\n",
        "    #the below code is used for multiprogramming\n",
        "    #the number of process depends upon the number of cores present System\n",
        "    #process is used to call multiprogramming\n",
        "    manager=multiprocessing.Manager() \t\n",
        "    p1=Process(target=firstprocess)\n",
        "    p2=Process(target=secondprocess)\n",
        "    p3=Process(target=thirdprocess)\n",
        "    p4=Process(target=fourthprocess)\n",
        "    p5=Process(target=fifthprocess)\n",
        "    #p1.start() is used to start the thread execution\n",
        "    p1.start()\n",
        "    p2.start()\n",
        "    p3.start()\n",
        "    p4.start()\n",
        "    p5.start()\n",
        "    #After completion all the threads are joined\n",
        "    p1.join()\n",
        "    p2.join()\n",
        "    p3.join()\n",
        "    p4.join()\n",
        "    p5.join()\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "gUSexazlne__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "files=os.listdir('asmFiles')\n",
        "filenames=Y['ID'].tolist()\n",
        "class_y=Y['Class'].tolist()\n",
        "class_bytes=[]\n",
        "sizebytes=[]\n",
        "fnames=[]\n",
        "for file in files:\n",
        "    statinfo=os.stat('asmFiles/'+file)\n",
        "    # split the file name at '.' and take the first part of it i.e the file name\n",
        "    file=file.split('.')[0]\n",
        "    if any(file == filename for filename in filenames):\n",
        "        i=filenames.index(file)\n",
        "        class_bytes.append(class_y[i])\n",
        "        # converting into Mb's\n",
        "        sizebytes.append(statinfo.st_size/(1024.0*1024.0))\n",
        "        fnames.append(file)\n",
        "asm_size_byte=pd.DataFrame({'ID':fnames,'size':sizebytes,'Class':class_bytes})\n",
        "print (asm_size_byte.head())"
      ],
      "metadata": {
        "id": "oZiLtDfxnyuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xtsne=TSNE(perplexity=50)\n",
        "results=xtsne.fit_transform(result_asm.drop(['ID','Class'], axis=1).fillna(0))\n",
        "vis_x = results[:, 0]\n",
        "vis_y = results[:, 1   ]\n",
        "plt.scatter(vis_x, vis_y, c=data_y, cmap=plt.cm.get_cmap(\"jet\", 9))\n",
        "plt.colorbar(ticks=range(10))\n",
        "plt.clim(0.5, 9)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ol4Mk02kn752"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xtsne=TSNE(perplexity=30)\n",
        "results=xtsne.fit_transform(result_asm.drop(['ID','Class', 'rtn', '.BSS:', '.CODE','size'], axis=1))\n",
        "vis_x = results[:, 0]\n",
        "vis_y = results[:, 1]\n",
        "plt.scatter(vis_x, vis_y, c=data_y, cmap=plt.cm.get_cmap(\"jet\", 9))\n",
        "plt.colorbar(ticks=range(10))\n",
        "plt.clim(0.5, 9)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4KSt_eBNoEXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "asm_y = result_asm['Class']\n",
        "asm_x = result_asm.drop(['ID','Class','.BSS:','rtn','.CODE'], axis=1)\n",
        "X_train_asm, X_test_asm, y_train_asm, y_test_asm = train_test_split(asm_x,asm_y ,stratify=asm_y,test_size=0.20)\n",
        "X_train_asm, X_cv_asm, y_train_asm, y_cv_asm = train_test_split(X_train_asm, y_train_asm,stratify=y_train_asm,test_size=0.20)\n",
        "alpha = [x for x in range(1, 21,2)]\n",
        "cv_log_error_array=[]\n",
        "for i in alpha:\n",
        "    k_cfl=KNeighborsClassifier(n_neighbors=i)\n",
        "    k_cfl.fit(X_train_asm,y_train_asm)\n",
        "    sig_clf = CalibratedClassifierCV(k_cfl, method=\"sigmoid\")\n",
        "    sig_clf.fit(X_train_asm, y_train_asm)\n",
        "    predict_y = sig_clf.predict_proba(X_cv_asm)\n",
        "    cv_log_error_array.append(log_loss(y_cv_asm, predict_y, labels=k_cfl.classes_, eps=1e-15))\n",
        "    \n",
        "for i in range(len(cv_log_error_array)):\n",
        "    print ('log_loss for k = ',alpha[i],'is',cv_log_error_array[i])\n",
        "\n",
        "best_alpha = np.argmin(cv_log_error_array)\n",
        "    \n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(alpha, cv_log_error_array,c='g')\n",
        "for i, txt in enumerate(np.round(cv_log_error_array,3)):\n",
        "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\n",
        "plt.grid()\n",
        "plt.title(\"Cross Validation Error for each alpha\")\n",
        "plt.xlabel(\"Alpha i's\")\n",
        "plt.ylabel(\"Error measure\")\n",
        "plt.show()\n",
        "\n",
        "k_cfl=KNeighborsClassifier(n_neighbors=alpha[best_alpha])\n",
        "k_cfl.fit(X_train_asm,y_train_asm)\n",
        "sig_clf = CalibratedClassifierCV(k_cfl, method=\"sigmoid\")\n",
        "sig_clf.fit(X_train_asm, y_train_asm)\n",
        "pred_y=sig_clf.predict(X_test_asm)\n",
        "\n",
        "\n",
        "predict_y = sig_clf.predict_proba(X_train_asm)\n",
        "print ('log loss for train data',log_loss(y_train_asm, predict_y))\n",
        "predict_y = sig_clf.predict_proba(X_cv_asm)\n",
        "print ('log loss for cv data',log_loss(y_cv_asm, predict_y))\n",
        "predict_y = sig_clf.predict_proba(X_test_asm)\n",
        "print ('log loss for test data',log_loss(y_test_asm, predict_y))\n",
        "plot_confusion_matrix(y_test_asm,sig_clf.predict(X_test_asm))"
      ],
      "metadata": {
        "id": "hIDmSDezoHMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = [10 ** x for x in range(-5, 4)]\n",
        "cv_log_error_array=[]\n",
        "for i in alpha:\n",
        "    logisticR=LogisticRegression(penalty='l2',C=i,class_weight='balanced')\n",
        "    logisticR.fit(X_train_asm,y_train_asm)\n",
        "    sig_clf = CalibratedClassifierCV(logisticR, method=\"sigmoid\")\n",
        "    sig_clf.fit(X_train_asm, y_train_asm)\n",
        "    predict_y = sig_clf.predict_proba(X_cv_asm)\n",
        "    cv_log_error_array.append(log_loss(y_cv_asm, predict_y, labels=logisticR.classes_, eps=1e-15))\n",
        "    \n",
        "for i in range(len(cv_log_error_array)):\n",
        "    print ('log_loss for c = ',alpha[i],'is',cv_log_error_array[i])\n",
        "\n",
        "best_alpha = np.argmin(cv_log_error_array)\n",
        "    \n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(alpha, cv_log_error_array,c='g')\n",
        "for i, txt in enumerate(np.round(cv_log_error_array,3)):\n",
        "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\n",
        "plt.grid()\n",
        "plt.title(\"Cross Validation Error for each alpha\")\n",
        "plt.xlabel(\"Alpha i's\")\n",
        "plt.ylabel(\"Error measure\")\n",
        "plt.show()\n",
        "\n",
        "logisticR=LogisticRegression(penalty='l2',C=alpha[best_alpha],class_weight='balanced')\n",
        "logisticR.fit(X_train_asm,y_train_asm)\n",
        "sig_clf = CalibratedClassifierCV(logisticR, method=\"sigmoid\")\n",
        "sig_clf.fit(X_train_asm, y_train_asm)\n",
        "\n",
        "predict_y = sig_clf.predict_proba(X_train_asm)\n",
        "print ('log loss for train data',(log_loss(y_train_asm, predict_y, labels=logisticR.classes_, eps=1e-15)))\n",
        "predict_y = sig_clf.predict_proba(X_cv_asm)\n",
        "print ('log loss for cv data',(log_loss(y_cv_asm, predict_y, labels=logisticR.classes_, eps=1e-15)))\n",
        "predict_y = sig_clf.predict_proba(X_test_asm)\n",
        "print ('log loss for test data',(log_loss(y_test_asm, predict_y, labels=logisticR.classes_, eps=1e-15)))\n",
        "plot_confusion_matrix(y_test_asm,sig_clf.predict(X_test_asm))"
      ],
      "metadata": {
        "id": "5XaYw-Cxon-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpha=[10,50,100,500,1000,2000,3000]\n",
        "cv_log_error_array=[]\n",
        "for i in alpha:\n",
        "    r_cfl=RandomForestClassifier(n_estimators=i,random_state=42,n_jobs=-1)\n",
        "    r_cfl.fit(X_train_asm,y_train_asm)\n",
        "    sig_clf = CalibratedClassifierCV(r_cfl, method=\"sigmoid\")\n",
        "    sig_clf.fit(X_train_asm, y_train_asm)\n",
        "    predict_y = sig_clf.predict_proba(X_cv_asm)\n",
        "    cv_log_error_array.append(log_loss(y_cv_asm, predict_y, labels=r_cfl.classes_, eps=1e-15))\n",
        "\n",
        "for i in range(len(cv_log_error_array)):\n",
        "    print ('log_loss for c = ',alpha[i],'is',cv_log_error_array[i])\n",
        "\n",
        "\n",
        "best_alpha = np.argmin(cv_log_error_array)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(alpha, cv_log_error_array,c='g')\n",
        "for i, txt in enumerate(np.round(cv_log_error_array,3)):\n",
        "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\n",
        "plt.grid()\n",
        "plt.title(\"Cross Validation Error for each alpha\")\n",
        "plt.xlabel(\"Alpha i's\")\n",
        "plt.ylabel(\"Error measure\")\n",
        "plt.show()\n",
        "\n",
        "r_cfl=RandomForestClassifier(n_estimators=alpha[best_alpha],random_state=42,n_jobs=-1)\n",
        "r_cfl.fit(X_train_asm,y_train_asm)\n",
        "sig_clf = CalibratedClassifierCV(r_cfl, method=\"sigmoid\")\n",
        "sig_clf.fit(X_train_asm, y_train_asm)\n",
        "predict_y = sig_clf.predict_proba(X_train_asm)\n",
        "print ('log loss for train data',(log_loss(y_train_asm, predict_y, labels=sig_clf.classes_, eps=1e-15)))\n",
        "predict_y = sig_clf.predict_proba(X_cv_asm)\n",
        "print ('log loss for cv data',(log_loss(y_cv_asm, predict_y, labels=sig_clf.classes_, eps=1e-15)))\n",
        "predict_y = sig_clf.predict_proba(X_test_asm)\n",
        "print ('log loss for test data',(log_loss(y_test_asm, predict_y, labels=sig_clf.classes_, eps=1e-15)))\n",
        "plot_confusion_matrix(y_test_asm,sig_clf.predict(X_test_asm))"
      ],
      "metadata": {
        "id": "jg85gDwcov92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpha=[10,50,100,500,1000,2000,3000]\n",
        "cv_log_error_array=[]\n",
        "for i in alpha:\n",
        "    x_cfl=XGBClassifier(n_estimators=i,nthread=-1)\n",
        "    x_cfl.fit(X_train_asm,y_train_asm)\n",
        "    sig_clf = CalibratedClassifierCV(x_cfl, method=\"sigmoid\")\n",
        "    sig_clf.fit(X_train_asm, y_train_asm)\n",
        "    predict_y = sig_clf.predict_proba(X_cv_asm)\n",
        "    cv_log_error_array.append(log_loss(y_cv_asm, predict_y, labels=x_cfl.classes_, eps=1e-15))\n",
        "\n",
        "for i in range(len(cv_log_error_array)):\n",
        "    print ('log_loss for c = ',alpha[i],'is',cv_log_error_array[i])\n",
        "\n",
        "\n",
        "best_alpha = np.argmin(cv_log_error_array)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(alpha, cv_log_error_array,c='g')\n",
        "for i, txt in enumerate(np.round(cv_log_error_array,3)):\n",
        "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\n",
        "plt.grid()\n",
        "plt.title(\"Cross Validation Error for each alpha\")\n",
        "plt.xlabel(\"Alpha i's\")\n",
        "plt.ylabel(\"Error measure\")\n",
        "plt.show()\n",
        "\n",
        "x_cfl=XGBClassifier(n_estimators=alpha[best_alpha],nthread=-1)\n",
        "x_cfl.fit(X_train_asm,y_train_asm)\n",
        "sig_clf = CalibratedClassifierCV(x_cfl, method=\"sigmoid\")\n",
        "sig_clf.fit(X_train_asm, y_train_asm)\n",
        "    \n",
        "predict_y = sig_clf.predict_proba(X_train_asm)\n",
        "\n",
        "print ('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train_asm, predict_y))\n",
        "predict_y = sig_clf.predict_proba(X_cv_asm)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv_asm, predict_y))\n",
        "predict_y = sig_clf.predict_proba(X_test_asm)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test_asm, predict_y))\n",
        "plot_confusion_matrix(y_test_asm,sig_clf.predict(X_test_asm))"
      ],
      "metadata": {
        "id": "xhXYxeHpo-kO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#xgboost classifier on final feature\n",
        "alpha=[10,50,100,500,1000,2000,3000]\n",
        "cv_log_error_array=[]\n",
        "for i in alpha:\n",
        "    x_cfl=XGBClassifier(n_estimators=i)\n",
        "    x_cfl.fit(X_train_merge,y_train_merge)\n",
        "    sig_clf = CalibratedClassifierCV(x_cfl, method=\"sigmoid\")\n",
        "    sig_clf.fit(X_train_merge, y_train_merge)\n",
        "    predict_y = sig_clf.predict_proba(X_cv_merge)\n",
        "    cv_log_error_array.append(log_loss(y_cv_merge, predict_y, labels=x_cfl.classes_, eps=1e-15))\n",
        "\n",
        "for i in range(len(cv_log_error_array)):\n",
        "    print ('log_loss for c = ',alpha[i],'is',cv_log_error_array[i])\n",
        "\n",
        "\n",
        "best_alpha = np.argmin(cv_log_error_array)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(alpha, cv_log_error_array,c='g')\n",
        "for i, txt in enumerate(np.round(cv_log_error_array,3)):\n",
        "    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\n",
        "plt.grid()\n",
        "plt.title(\"Cross Validation Error for each alpha\")\n",
        "plt.xlabel(\"Alpha i's\")\n",
        "plt.ylabel(\"Error measure\")\n",
        "plt.show()\n",
        "\n",
        "x_cfl=XGBClassifier(n_estimators=3000,nthread=-1)\n",
        "x_cfl.fit(X_train_merge,y_train_merge,verbose=True)\n",
        "sig_clf = CalibratedClassifierCV(x_cfl, method=\"sigmoid\")\n",
        "sig_clf.fit(X_train_merge, y_train_merge)\n",
        "\n",
        "predict_y = sig_clf.predict_proba(X_train_merge)\n",
        "print ('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train_merge, predict_y))\n",
        "predict_y = sig_clf.predict_proba(X_cv_merge)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv_merge, predict_y))\n",
        "predict_y = sig_clf.predict_proba(X_test_merge)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test_merge, predict_y))"
      ],
      "metadata": {
        "id": "Frh2EYV2pHY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_cfl=XGBClassifier()\n",
        "\n",
        "prams={\n",
        "    'learning_rate':[0.01,0.03,0.05,0.1,0.15,0.2],\n",
        "     'n_estimators':[100,200,500,1000,2000],\n",
        "     'max_depth':[3,5,10],\n",
        "    'colsample_bytree':[0.1,0.3,0.5,1],\n",
        "    'subsample':[0.1,0.3,0.5,1]\n",
        "}\n",
        "random_cfl=RandomizedSearchCV(x_cfl,param_distributions=prams,verbose=10,n_jobs=-1,)\n",
        "random_cfl.fit(X_train_merge, y_train_merge)"
      ],
      "metadata": {
        "id": "iBoIENtopWux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# default paramters\n",
        "# class xgboost.XGBClassifier(max_depth=3, learning_rate=0.1, n_estimators=100, silent=True, \n",
        "# objective='binary:logistic', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, \n",
        "# max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, \n",
        "# scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)\n",
        "\n",
        "# some of methods of RandomForestRegressor()\n",
        "# fit(X, y, sample_weight=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None)\n",
        "# get_params([deep])\tGet parameters for this estimator.\n",
        "# predict(data, output_margin=False, ntree_limit=0) : Predict with data. NOTE: This function is not thread safe.\n",
        "# get_score(importance_type='weight') -> get the feature importance\n",
        "# -----------------------\n",
        "\n",
        "x_cfl=XGBClassifier(n_estimators=1000,max_depth=10,learning_rate=0.15,colsample_bytree=0.3,subsample=1,nthread=-1)\n",
        "x_cfl.fit(X_train_merge,y_train_merge,verbose=True)\n",
        "sig_clf = CalibratedClassifierCV(x_cfl, method=\"sigmoid\")\n",
        "sig_clf.fit(X_train_merge, y_train_merge)\n",
        "    \n",
        "predict_y = sig_clf.predict_proba(X_train_merge)\n",
        "print ('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train_merge, predict_y))\n",
        "predict_y = sig_clf.predict_proba(X_cv_merge)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv_merge, predict_y))\n",
        "predict_y = sig_clf.predict_proba(X_test_merge)\n",
        "print('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test_merge, predict_y))\n",
        "plot_confusion_matrix(y_test_asm,sig_clf.predict(X_test_merge))"
      ],
      "metadata": {
        "id": "UZYur8jApfb5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}